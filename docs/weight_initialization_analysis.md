# Weight Initialization Analysis: Gradient Explosion in dia_008

**Date:** December 17, 2025  
**Issue:** Gradient explosions when training large Dia architecture from scratch on 83k sample dataset

---

## Executive Summary

The gradient explosion observed in `dia_008` was caused by the **combination** of:
1. Large model architecture (~1.5B parameters, 18 decoder layers)
2. Large diverse dataset (83k samples)
3. Training from scratch with Xavier initialization

This combination creates a "perfect storm" where random initialization + high gradient variance from diverse data leads to unstable training dynamics.

---

## Evidence

| Experiment | Architecture | Dataset Size | From Scratch | Result |
|------------|-------------|--------------|--------------|--------|
| dia_003 | Small (~50M) | 40 samples | ‚úÖ | ‚úÖ Converged |
| dia_006 | Small (~200M) | 83k samples | ‚úÖ | ‚úÖ Converged (underparam) |
| dia_007 | Small (~200M) | 83k samples | ‚úÖ | ‚úÖ Converged |
| **dia_008** | **Large (~1.5B)** | **83k samples** | **‚úÖ** | **‚ùå Exploded @ 800 steps** |
| dia_009 | Large (~1.5B) | Small (overfit) | ‚úÖ | ‚úÖ Converged |

**Key observation:** dia_009 proves the large architecture CAN train from scratch‚Äîbut only on small datasets.

---

## Root Cause Analysis

### 1. Xavier Initialization Doesn't Scale to Depth

The current `_init_weights()` uses Xavier uniform initialization:

```python
def _init_weights(self):
    for module in self.modules():
        if isinstance(module, DenseGeneral):
            torch.nn.init.xavier_uniform_(module.weight)
```

**Problem:** Xavier initialization maintains variance across a **single layer**, but doesn't account for **depth**. With 18 decoder layers, residual connections compound:

```
Layer 1:  x‚ÇÅ = x‚ÇÄ + f‚ÇÅ(x‚ÇÄ)     # Variance grows by ~1x
Layer 2:  x‚ÇÇ = x‚ÇÅ + f‚ÇÇ(x‚ÇÅ)     # Variance grows by ~2x  
...
Layer 18: x‚ÇÅ‚Çà = x‚ÇÅ‚Çá + f‚ÇÅ‚Çà(x‚ÇÅ‚Çá) # Variance grows by ~18x!
```

This causes gradients to explode during backpropagation as they flow through the residual paths.

### 2. Large Dataset Creates High Gradient Variance

With 83k diverse samples:
- Each mini-batch contains samples from very different distributions
- Gradients point in vastly different directions batch-to-batch
- Random initialization has no "anchor" to stabilize learning

Compare to finetuning, where pretrained weights provide:
- A stable starting point in a good loss basin
- Consistent feature representations that align gradients

### 3. Gradient Clipping Doesn't Fix Direction

Even with `GRAD_CLIP_MAX_NORM = 5.0`:
- Clipping controls **magnitude** but not **direction**
- Inconsistent gradient directions from diverse batches still cause oscillation
- The model weight updates become effectively random walks

---

## Why Pretrained Weights Work

When finetuning from pretrained weights:

1. **Weights are in a "good basin"** ‚Äî loss landscape is smooth locally
2. **Feature representations are aligned** ‚Äî similar inputs produce similar gradients
3. **Attention patterns are established** ‚Äî no need to learn basic structure
4. **Embeddings are meaningful** ‚Äî not random noise

When training from scratch:

1. **All weights are random** ‚Äî high-dimensional random walk
2. **Embeddings are noise** ‚Äî model must learn vocabulary simultaneously
3. **Attention is uniform** ‚Äî Q, K, V projections produce meaningless scores
4. **Loss landscape is rugged** ‚Äî many local minima and saddle points

---

## Proposed Solutions

### Solution 1: Depth-Scaled Initialization (GPT-2/GPT-3 Style)

Scale residual path outputs by `1/‚àö(2N)` where N = number of layers:

```python
def _init_weights(self):
    n_layers = self.config.model.decoder.n_layer
    
    for name, module in self.named_modules():
        if isinstance(module, DenseGeneral):
            # Scale down residual path outputs (o_proj in attention, wo in MLP)
            if 'o_proj' in name or 'wo' in name:
                std = 0.02 / math.sqrt(2 * n_layers)
                torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            elif getattr(module, "use_glu_he_init", False):
                # Existing He init for SwiGLU gates
                fan_in = module.in_shapes[0] if module.in_shapes else 1
                bound = math.sqrt(6.0 / fan_in)
                module.weight.uniform_(-bound, bound)
            else:
                torch.nn.init.xavier_uniform_(module.weight)
        
        elif isinstance(module, torch.nn.Embedding):
            # Smaller embedding init for large vocab
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

**Rationale:** This is the initialization scheme used by GPT-2/GPT-3 and has proven stable for very deep transformers.

### Solution 2: Extended Warmup Period

For 83k samples √ó 15 epochs ‚âà 1.2M steps:

| Current | Recommended |
|---------|-------------|
| 2,000 warmup steps | 10,000-20,000 warmup steps |
| ~0.17% of training | ~1-2% of training |

Longer warmup allows:
- Gradients to stabilize before aggressive updates
- Learning rate to reach full value after model has found a stable region

### Solution 3: Lower Initial Learning Rate

| Current | Recommended |
|---------|-------------|
| 1e-4 (after warmup) | 5e-5 or 1e-5 (after warmup) |

Lower learning rate reduces:
- Magnitude of weight updates
- Sensitivity to gradient noise
- Risk of jumping out of stable regions

### Solution 4: Curriculum Training (Progressive Dataset Scaling)

```
Phase 1: Train on 5k samples until loss < 5.0 (~500 steps)
Phase 2: Train on 20k samples until loss < 5.5 (~2000 steps)  
Phase 3: Train on full 83k samples
```

This allows the model to:
- Learn basic structure on consistent data first
- Build stable representations before encountering diversity
- Avoid the "cold start" problem of random init + high variance

### Solution 5: Pre-LayerNorm Architecture

Switch from Post-LN to Pre-LN transformer blocks:

```python
# Post-LN (current, less stable)
x = x + attention(x)
x = layer_norm(x)

# Pre-LN (more stable for deep networks)
x = x + attention(layer_norm(x))
```

Pre-LN has been shown to stabilize training for very deep transformers.

---

## Recommended Action Plan

### Immediate (Low Risk)
1. ‚úÖ Increase warmup to 10,000 steps
2. ‚úÖ Lower learning rate to 5e-5

### Medium Term (Requires Code Changes)
3. üîß Implement depth-scaled initialization
4. üîß Add gradient norm monitoring per-layer to identify explosion source

### Long Term (Architecture Changes)
5. üèóÔ∏è Consider Pre-LN architecture for future from-scratch training
6. üèóÔ∏è Add optional QK-Norm for attention stability

---

## Implementation: Depth-Scaled Init

Add to `dia/layers.py`:

```python
def _init_weights(self):
    """Initialize weights with depth scaling for stable training from scratch."""
    enc_layers = self.config.model.encoder.n_layer
    dec_layers = self.config.model.decoder.n_layer
    
    for name, module in self.named_modules():
        # Determine which component this belongs to
        is_encoder = 'encoder' in name
        n_layers = enc_layers if is_encoder else dec_layers
        
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1d)):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
                
        elif isinstance(module, DenseGeneral):
            # Residual output projections get scaled init
            if any(proj in name for proj in ['o_proj', 'wo']):
                std = 0.02 / math.sqrt(2 * n_layers)
                with torch.no_grad():
                    module.weight.normal_(mean=0.0, std=std)
            elif getattr(module, "use_glu_he_init", False):
                fan_in = module.in_shapes[0] if module.in_shapes else 1
                bound = math.sqrt(6.0 / fan_in)
                with torch.no_grad():
                    module.weight.uniform_(-bound, bound)
            else:
                torch.nn.init.xavier_uniform_(module.weight)
            
            if getattr(module, "bias", None) is not None:
                torch.nn.init.zeros_(module.bias)
                
        elif isinstance(module, torch.nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            
        elif isinstance(module, (torch.nn.LayerNorm, torch.nn.modules.normalization.RMSNorm)):
            if hasattr(module, "weight") and module.weight is not None:
                torch.nn.init.ones_(module.weight)
            if hasattr(module, "bias") and module.bias is not None:
                torch.nn.init.zeros_(module.bias)
```

---

## References

1. **GPT-2 Paper** - Radford et al. "Language Models are Unsupervised Multitask Learners" (2019)
   - Section on weight initialization scaling

2. **On Layer Normalization in the Transformer Architecture** - Xiong et al. (2020)
   - Analysis of Pre-LN vs Post-LN stability

3. **Fixup Initialization** - Zhang et al. (2019)
   - Depth-scaled initialization theory

4. **ŒºP (Maximal Update Parameterization)** - Yang et al. (2022)
   - Hyperparameter transfer across model scales

---

## Appendix: Experiment Configs

### dia_008 (Failed)
```json
{
  "architecture": "18 decoder layers, 2048 embed dim",
  "dataset": "83k samples",
  "batch_size": 4,
  "warmup_steps": 2000,
  "learning_rate": 1e-4,
  "result": "Gradient explosion @ 800 steps"
}
```

### dia_009 (Succeeded)
```json
{
  "architecture": "18 decoder layers, 2048 embed dim",
  "dataset": "Small overfit test",
  "batch_size": 1,
  "warmup_steps": 0,
  "learning_rate": 1e-4,
  "result": "Converged successfully"
}
```

