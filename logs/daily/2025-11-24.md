# Research Log: 2025-11-24

## Overview
Updated delay pattern to match MusicGen (012345678) and worked through dataset/collate issues that were preventing proper training.

---

## Experiments

### Bug fixes (3:00am)
Asked LLMs to identify pressing bugs, found some low cost bugs which I updated and pushed to GitHub.

### Delay pattern update (3:40am)
**Change:** Updated delay pattern to match MusicGen 012345678  
**Hypothesis:** This will improve ability to learn melody. Before, codebook 0 was emphasized due to next codebooks being generated much later in sequence - good for bass/drums but not melody.  
**Result:** Have to train from scratch now with new delay pattern

### Overfit tests (10:19pm - 11:31pm)

**Test 1:** Basic overfit on 1 sample
- Basically random noise almost entire time
- Step 949: ~1/4 through 7sec sample started rendering perfect quality output. Other demo at same timestep was just noise
- Step 1099: Same pattern - 1/4 noise, 3/4 sample
- Steps 1199, 1299, 1399: Same pattern continued
- Pattern: Switched between pure noise OR 1/4 noise + 3/4 sample. Never gradual.

**Test 2:** Updated demos to CFG=1, prompt to match sample exactly
- Got WORSE - pure noise up to 1699 steps despite similar training curves
- Step 1749: first 1/4 pure noise, last 3/4 was sample
- Pure noise until 2199: full sample
- Step 2249: pure noise, Step 2299: mostly sample

**Test 3:** Took out random window
- Performed exact same as before

**Test 4:** Testing unconditional_train=1
**Hypothesis:** If this fixes noise problem, might be encoder issue  
**Config:** `unconditional_frac=1, batch=1, lr=1e-4, warmup=0`

```bash
python -m dia.finetune_acc --config ./dia/config_overfit.json --scratch \
  --output_dir /nfs/turbo/smtd-hwdong/ocamp/DONT_DELETE/new_delay_overfit \
  --preencoded_dir /home/ocamp/dia-new-delay/1sampledata/test1/preprocessedsingle \
  --batch_size 1 --grad_accum_steps 1 --unconditional_frac 1 \
  --eval_every_epochs 10 --demo_every_epochs 50 --epochs 10000 \
  --run_name new_delay_overfit --save_every 100 --learning_rate 1e-4 \
  --warmup_steps 0 --save_after_epoch 500 --force_single_gpu
```

**Result:** First music at step 2049 (seed 43), step 2099 (seed 42) - perfect original sample. Similar results to previous → NOT an encoder problem.

**Test 4b:** Eval demos with temperature=0, still unconditional
- Step 249: Started hearing music with some noise at beginning
- Step 399: Both seeds rendering perfect samples, no noise, consistent

**Theory:** The model is simultaneously learning embeddings. Since we start from scratch, all embeddings are randomized. But since we're just overfitting on one sample, most embeddings are unchanged - we're overfitting just a subset. When we do higher temperature, it selects some random embeddings and outputs noise.

- Model wouldn't bounce between sample and noise - it was either all sample or all noise until first sample token hit, then all sample
- Once 1 sample token selected, model likely selects only sample tokens after (probabilities greatly outweigh random ones)
- Before with high temp: model randomly selecting between many tokens → low likelihood of selecting first sample token
- Now with low temp: much more deterministic

**Next Steps:** Need to fill up the embedding space by increasing dataset and training longer. Start with ~40 samples, unconditional first then conditional. Keep demos at temp=0 and add temp=0.5 and temp=1 to test embedding learning over time. Added: vocabulary coverage tracking, output entropy logging, multi-temperature demos

### 40 Suey Loop Kit tests

**Test 1 - Unconditional:** `batch=1, unconditional_frac=1, lr=1e-4`  
**Dataset:** 40 samples (472,320 tokens, 1024/1024 vocab coverage)

```bash
python -m dia.finetune_acc --config ./dia/config_overfit.json --scratch \
  --output_dir /nfs/turbo/smtd-hwdong/ocamp/DONT_DELETE/new_delay_overfit \
  --preencoded_dir /home/ocamp/dia-new-delay/1sampledata/test1/preprocessed \
  --batch_size 1 --grad_accum_steps 1 --unconditional_frac 1 \
  --eval_every_epochs 10 --demo_every_epochs 50 --epochs 10000 \
  --run_name new_delay_overfit --save_every 100 --learning_rate 1e-4 \
  --warmup_steps 0 --save_after_epoch 500 --force_single_gpu
```

**Results:**
- Epoch 50: temp=0 buzzing (non-noise!), temp=0.5/1 noise. Output entropy decreasing. Eval loss began increasing as soon as training started.
- Epoch 100: Similar, but temp=1 and temp=0.5 not just pure noise anymore. Eval loss still increasing.
- Epoch 150: temp=0 outputs <1sec, barely any noise. temp=1 slight muffling. temp=0.5 one muffled, one <1sec. Eval loss rate of change increasing upwards.
- Epoch 200: Barely any differences between temperatures. Lower temps quieter output though. Muffling. Eval loss still increasing. Gradient clipping at 5 pretty consistently.
- Epoch 250: Temp correlated more to loudness of muffling. Still just noise from all three temps. Slight decrease in grad norm clipping %. Train/loss below 1 but averaging around 1.3 now.
- Epoch 300: Just noise still. Loss average around 0.77 but most down to 0.03-0.15 range. Output entropy down to below 0 (from above 6 at peak). Eval loss still increasing, around 9.5 now.
- Epoch 350: Just noise still. Loss average around 0.4 from curve weighting, mainly ranges around 0.1. Entropy hovering around 0.4.
- Epoch 400: Just noise still. Loss hover around 0.03 now. Eval still increasing above 11 now. Grad norm decreased a bit, less clipping.

**Test 2 - Conditional 0.15:** `batch=1, unconditional_frac=0.15, lr=1e-4`
- Epoch 50: Eval loss start 6.7, end 6.6. Entropy decreasing. Grad norm not clipping. temp=1 noise, temp=0.5 click fade out to silence, temp=0 more prominent version of 0.5.
- Epoch 100: Grad norm increasing not clipping badly yet. Eval back up to 6.69. Loss decreasing but more variance than earlier. temp=0 buzzing noise, other temps noise quieter.
- Epoch 150: Eval 6.73. temp=0 slightly more interesting buzzing, volume changes mostly. Other temps noise. Output entropy decreasing still. Gradients clipping consistently now at 5.
- Epoch 200: Eval 7.55. Buzzing fading.

**Theory:** Batch size might be introducing too much variance

**Test 2b - Unconditional batch20:** `batch=20, unconditional_frac=1, lr=1e-4`

```bash
python -m dia.finetune_acc --config ./dia/config_overfit.json --scratch \
  --output_dir /nfs/turbo/smtd-hwdong/ocamp/DONT_DELETE/new_delay_overfit \
  --preencoded_dir /home/ocamp/dia-new-delay/1sampledata/test1/preprocessed \
  --batch_size 20 --grad_accum_steps 1 --unconditional_frac 1 \
  --eval_step 20 --demo_every 100 --epochs 10000 \
  --run_name new_delay_overfit --save_every 100 --learning_rate 1e-4 \
  --warmup_steps 0 --save_after_epoch 500
```

- Epoch 100: Eval 6.75→6.6. Much smoother loss, max gradient 0.5, noisy samples.
- Epoch 200: Eval up to 6.76.
- Epoch 300: Eval up to 7. Pure noise.
- Epoch 400: Eval up to 7.4. Pure noise.

**Test 2c - Conditional batch20:** `batch=20, unconditional_frac=0, lr=1e-4`  
**W&B:** https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/j9nl1at8

```bash
python -m dia.finetune_acc --config ./dia/config_overfit.json --scratch \
  --output_dir /nfs/turbo/smtd-hwdong/ocamp/DONT_DELETE/new_delay_overfit \
  --preencoded_dir /home/ocamp/dia-new-delay/1sampledata/test1/preprocessed \
  --batch_size 20 --grad_accum_steps 1 --unconditional_frac 0 \
  --eval_step 20 --demo_every 100 --epochs 10000 \
  --run_name new_delay_overfit --save_every 100 --learning_rate 1e-4 \
  --warmup_steps 0 --save_after_epoch 500
```

- Epoch 100: Eval start 6.75, end 6.67. Smooth train loss. temp=0 quiet buzzing, rest quiet noise.
- Started with same exact pattern as before → killed it

**Conclusion:** Eval loss is irrelevant! Was paying too close attention to that (facepalm)

**Theory:** Just not running it long enough, so started it again and ran it overnight. Ran for 50 minutes. Results: loss converged to 0.02, demos still just noise.

### Removing tags and fixing CFG
**W&B:** https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/ycenomr9  
**Hypothesis:** With unconditional_frac at 0 but cfg at 4 for eval demos, it was still incorporating unconditional weights which messes up demos since unconditional learning was essentially still random. Also adding --tag_no_shuffle to simplify training and reduce generalization.

```bash
python -m dia.finetune_acc \
  --config ./dia/config_overfit.json --scratch \
  --output_dir /nfs/turbo/smtd-hwdong/ocamp/DONT_DELETE/new_delay_overfit \
  --preencoded_dir /home/ocamp/dia-new-delay/1sampledata/test1/preprocessed \
  --batch_size 20 --grad_accum_steps 1 --unconditional_frac 0 \
  --eval_step 20 --demo_every 100 --epochs 10000 \
  --run_name new_delay_overfit --save_every 100 --learning_rate 1e-4 \
  --warmup_steps 0 --save_after_epoch 500 --tag_no_shuffle
```

**Result:** Epoch 100: very smooth train/loss. Very unsuccessful.

**Theory:** Must be some dataset or collate issue. As soon as I changed the collate/dataset from the sliding window code, it broke.

### Removing special tokens from loss calculation
**Change:** Removed special tokens from loss calculation  
**Result:** Had no relevant change from previous - root issue still failing

### Reverted dataset/collate - BREAKTHROUGH
**W&B:** https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/e8bcybi2  
**Runtime:** 6h 51m 42s (overnight)  
**Config:** `configs/experiments/20251124_dia_003.json`

**Change:** Reverted somewhat back to old implementation of dataset/collate

**Results:**
- Gradients rarely ever clipped
- Steady increase log-type graph reaching max around 30k steps
- Eval decreased until ~8000 steps then increased consistently until training stopped at 10000 epochs (ended ~9.3)
- Train/loss finished around 3.5, consistently decreased entire time
- Output entropy consistently decreased
- Audio files clearly sound like training data ✅

**Why it worked:**
1. The code update bringing back the old sliding window implementation fixed it
2. Previous dataset where it would learn the same 40 samples was way too small
3. Overfitting on 1 sample was likely outlier case - can just change all params to optimize for same token sequence
4. With small dataset, gradients are too big causing grad norm to skyrocket → lose important gradients due to clipping → don't actually learn anything

**The Fix (sliding window logic was inverted):**

Old (broken) logic:
- `use_sliding_window=True`: return full sequence, let collate_fn do random cropping
- `use_sliding_window=False`: fixed crop from start (deterministic)

New (working) logic:
- `use_sliding_window=True`: random crop in Dataset (data augmentation, efficient)
- `use_sliding_window=False`: fixed crop from start (deterministic)

**Reminder TODO:** Confirm loss function that excludes special tokens since we added that and later took it out.

---

## Key Insights
- Temperature=0 is critical for assessing model learning when training from scratch (random embeddings)
- Model learns embeddings subset during overfit - higher temp samples from random unlearned embeddings → noise
- Once first "learned" token selected, model stays on track (probabilities dominate)
- Eval loss is NOT a good indicator of generation quality during from-scratch training
- Need to fill embedding space with more data and longer training
- Sliding window logic inversion was causing deterministic cropping (always first ~10 sec) → artificially low loss

---

## Commits
