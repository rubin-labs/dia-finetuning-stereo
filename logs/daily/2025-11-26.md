# Research Log: 2025-11-26

## Overview
Scaled up to larger dataset (~256 hours) with MusicGen delay pattern. Found model architecture was too small and fixed bottleneck bug.

---

## Experiments

### dia_006: MusicGen delay on larger dataset
**W&B:** https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/l2fzeu4u  
**Hypothesis:** Will get good results with musicgen delay pattern on larger dataset. Good test to see quality it can do with new delay through balancing bass + drums + melody.

**Setup:**
- ~84k samples (~80% tagged)
- 256 hour dataset
- MusicGen delay pattern
- Smaller architecture to account for dataset size
- Sliding window active (though most audio is short anyway)

**Findings:** Model was likely just way too small. Don't think this was the new delay making mistakes, just don't think the model has enough capacity to perform well at less than 200M parameters, which is far far lower than the original dia architecture I was working with before. Also not sure if I scaled down the model correctly either.

**Observations:** The significant reduction in model depth (18→6 layers) and width (2048→1024 dim) on a larger dataset (83k samples) resulted in convergence to poor perceptual quality, despite healthy loss curves.

**Conclusion:** The small architecture is underparameterized for the entropy of the ~230 hour dataset. Future runs must prioritize model capacity over training speed for datasets of this magnitude.

006 architecture is 4-5x smaller than the baseline, making it harder to synthesize long form structure even if token level cross entropy looks good.

**Next steps:** Keep everything the same, but revert back to the standard architecture.

---

### dia_007: Bottleneck fix
**W&B Run:** https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/groups/decoder_head_ablation_test/runs/aefzbmgb  
**W&B Group:** https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/groups/decoder_head_ablation_test/workspace

**Change:** Fixed issue where embedding dimension was 1024 and total attention embeddings were only 512, so we were downscaling to do the attention computations, then upscaling back to our 1024 embeddings, essentially adding a compression layer to the model results.

**Hypothesis:** This accidental 0.5x compression was detrimental

**Results:** Our results proved that this was indeed detrimental. Although the loss curves and eval loss and entropy graphs were all very similar to each other, when I played audios side by side from various checkpoints, I could subjectively notice that the 007 model, without the accidental 0.5x compression, was better and retained more information.

**Overall though:** The model does not have nearly enough parameters to learn the dataset. It learns drums, but that's about it. It has a lot of trouble learning actual melody.

**Checkpoint eval results:** Best stability, fewer consistency outliers, decent clap scores, good key consistency (THIS IS ALL RELATIVE. IN REALITY ITS NOT GREAT BUT BASED ON COMPARISONS)

**Next steps:** I think I'll just increase the model back up to standard architecture to see what happens, and generally be able to compare to my previous type beat train from a few weeks ago to see if the delay is generally better, but also could be based on increased dataset so it'll be a little unclear but I'm expecting good results.

---

### dia_tpu_001: TPU initialization
**Command:**
```bash
PT_XLA_DEBUG=0 PYTHONPATH=. accelerate launch dia_tpu/finetune_acc_tpu.py \
  --batch_size 1 --audio_folder /home/olivercamp/local_data/hidenseek \
  --seed 42 --grad_accum_steps 1 --learning_rate 1e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --unconditional_frac 1 --no_decay_embed \
  --epochs 10000 --output_dir output
```

**Test 0's:** All failed. Asked LLM to cut a lot of stuff and only keep the core stuff by asking: if you could only keep 50% of the code, what would you keep. That helped it identify code that wasn't as necessary.

**Test 1:** This finally ran!  
**W&B:** https://wandb.ai/ocamp-university-of-michigan/audio-finetuning/runs/8ch0nam8  
Stopped cause taking too long.

**Test 2:**
```bash
PT_XLA_DEBUG=0 PYTHONPATH=. accelerate launch dia_tpu/finetune_acc_tpu.py \
  --batch_size 1 --audio_folder /home/olivercamp/local_data/hidenseek \
  --seed 42 --grad_accum_steps 1 --learning_rate 1e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --unconditional_frac 1 --no_decay_embed \
  --epochs 10000 --output_dir output --no_sliding_window --audio_length 600
```

---

## Key Insights
- Model architecture too small (4-5x reduction) → underparameterized for dataset size
- Bottleneck bug (1024→512→1024 compression) was detrimental to quality
- Need to scale back up to standard architecture

---

## Commits
- `00:02` [`2c520af`] **main**: 20251126_dia_006_musicgen_delay_larger_dataset (13 files)
- `00:17` [`03564ab`] **main**: 20251126_dia_006_musicgen_delay_larger_dataset (5 files)
- `14:53` [`b706c50`] **main**: Requirements.txt (15 files)
- `14:54` [`95d6c02`] **main**: 20251126_dia_007_bottleneck_fix_006ablationA (3 files)
- `14:57` [`b1c48e1`] **main**: Requirements.txt (2 files)
- `14:58` [`71f237a`] **main**: requirements.txt (3 files)
- `15:00` [`7600ee0`] **main**: requirements.txt (4 files)
- `15:15` [`c5e289d`] **main**: Requirements.txt (4 files)
- `15:16` [`277ed2d`] **main**: acc dependency (3 files)
- `15:18` [`5517a43`] **main**: dependencies (3 files)
- `15:19` [`c02cae4`] **main**: dep (3 files)
- `15:22` [`f5e5782`] **main**: dependencies (3 files)
- `15:25` [`fa887b7`] **main**: deps (3 files)
- `15:28` [`1d05223`] **main**: deps (2 files)
- `15:29` [`27ea419`] **main**: deps (3 files)
- `15:31` [`a7c2631`] **main**: deps (3 files)
- `15:36` [`a8293ce`] **main**: deps (3 files)
- `15:58` [`c1e4a97`] **main**: dia_tpu (4 files)
- `16:10` [`1928f8a`] **main**: 20251126_dia_tpu_001_init (7 files)
- `16:22` [`b79d986`] **main**: 20251126_dia_tpu_001_init (3 files)
- `16:44` [`89fe6bb`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:02` [`6e7d177`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:06` [`671c169`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:09` [`f10013a`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:14` [`68d8624`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:17` [`1c96ccd`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:22` [`a61ab13`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:39` [`0abc25f`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:44` [`e10a8ab`] **main**: 20251126_dia_tpu_001_init (3 files)
- `17:51` [`9f71e2f`] **main**: 20251126_dia_tpu_001_init (3 files)
- `18:01` [`91d984e`] **main**: 20251126_dia_tpu_001_init (3 files)
- `18:04` [`ba1c554`] **main**: 20251126_dia_tpu_001_init (3 files)
- `18:30` [`99dd7c2`] **main**: 20251126_dia_tpu_001_init (4 files)
- `21:07` [`13cf6ea`] **main**: 20251126_dia_tpu_001_init (3 files)
- `22:14` [`309e1da`] **main**: 20251126_dia_tpu_001_init (6 files)
- `22:24` [`1abe1e4`] **main**: 20251126_dia_tpu_001_init (3 files)
- `22:30` [`c62c0eb`] **main**: 20251126_dia_tpu_001_init (3 files)
- `22:31` [`9b66540`] **main**: 20251126_dia_tpu_001_init (3 files)
- `22:35` [`1ee43ae`] **main**: 20251126_dia_tpu_001_init (3 files)
- `22:49` [`f5fea7c`] **main**: 20251126_dia_tpu_001_init (3 files)
- `22:58` [`9fe7408`] **main**: 20251126_dia_tpu_001_init (3 files)
- `23:06` [`d5026e8`] **main**: 20251126_dia_tpu_001_init (3 files)
- `23:18` [`b831536`] **main**: 20251126_dia_tpu_001_init (3 files)
- `23:24` [`d1542fb`] **main**: 20251126_dia_tpu_001_init (3 files)
- `23:34` [`d7433f1`] **main**: 20251126_dia_tpu_001_init (3 files)
- `23:52` [`6254846`] **main**: 20251126_dia_tpu_001_init (3 files)
- `23:55` [`32945ba`] **main**: 20251126_dia_tpu_001_init (3 files)
