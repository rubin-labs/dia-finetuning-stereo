20251127_dia_010_gpu_refactor_scratch_dataset
    RUN 1
    went horribly! the depth scaling did not work on the larger paramter model and larger dataset. i THINK this is the issue but it very well might not be! I am going to conduct an A/B test right now, by reverting back to regular Xavier weighting to see if it reverts back to normal. If it doesnt, then this wasn't the issue! I trained for about 28000 steps so will likely train for that amount with the next one.
    Here is the wandb: https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/akrzvvpr?nw=nwuserocamp
    eval_loss is LITERALLY flat!. That cant be good. Training is also SUPER flat compared to previous Musicgen Delay runs. Output entropy is also very inconstnently going down, where usuually its more stable. Grad NORM i also think wasnt clipping??? I think that mightve gotten taken out i will have to chekc on that, but it did not vanish which is good. Since im doign a strict A/B test i wont cahnge the clipping at all to see but later might change it.

    RUN 2
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}
    LOOK BACK IN GITHUB FOR THE PREVIOUS RUN CONFIGS. not making new one just reusing 010.
    Reusing same configs, literally all i changed was i reverted back to the Xavier weighting.
    Wandb: https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/zddkoxqk?nw=nwuserocamp
    eval loss did decrease, however train loss and grad norm stayed very similar to eachother, therefore we can conclude that this is not the overarching issue. im hypothesising that the main issue has to do with adding back special tokens during training so for the next test i mask out special tokens during loss calculation.
    
    RUN 3
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}_{NoSpecialToks}
    masking out special tokens for loss calculations
    wandb: https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/yzr4nm4s
    train/loss was very similar to the previous 2 runs. eval/loss was higher but it was because it still was using special tokens in its loss calculation which was a mistake.
    
    RUN 4
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}_{NoSpecialToks}_{revertedto008}
    error after demo

    RUN 5
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}_{NoSpecialToks}_{revertedto008 + model.py fix}
    https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/groups/20251127_dia_010_gpu_refactor_scratch_dataset/runs/50ddhju7?nw=nwuserocamp

    still clipping

    RUN 6
    **Pretraining Code Prep**
    Added proper pretraining support to handle gradient explosion with large architecture:
    
    **Code Changes:**
    - `train_acc_gpu.py`: 
      - Added `--grad_clip` CLI arg (default 1.0, was hardcoded 5.0)
      - Added `--grad_clip_warmup` for gradual clip threshold increase
      - Added `--depth_scaling` flag for residual output scaling
      - Added `grad_norm/clip_ratio` metric to track clipping aggressiveness
      - Added `grad_norm/clip_threshold` to wandb logging
    - `layers.py`:
      - Updated `_init_weights()` with depth-scaled initialization
      - Scales o_proj and MLP wo by `1/sqrt(2*depth)` for stability
      - Scales final logits projection by 0.1 for stable initial outputs
    
    **Recommended Pretraining Config (18 decoder layers, 2048 embed):**
    ```
    --scratch
    --depth_scaling
    --grad_clip 1.0
    --grad_clip_warmup
    --learning_rate 3e-4
    --warmup_steps 2000
    --weight_decay 0.1
    ```
    
    **Key insight:** Constant clipping at 5.0 (400 pre-clip â†’ 5 post-clip) means LR is too high.
    Lower grad_clip forces smaller effective LR during instability.
    