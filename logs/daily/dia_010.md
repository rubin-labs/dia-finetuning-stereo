20251127_dia_010_gpu_refactor_scratch_dataset
    RUN 1
    went horribly! the depth scaling did not work on the larger paramter model and larger dataset. i THINK this is the issue but it very well might not be! I am going to conduct an A/B test right now, by reverting back to regular Xavier weighting to see if it reverts back to normal. If it doesnt, then this wasn't the issue! I trained for about 28000 steps so will likely train for that amount with the next one.
    Here is the wandb: https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/akrzvvpr?nw=nwuserocamp
    eval_loss is LITERALLY flat!. That cant be good. Training is also SUPER flat compared to previous Musicgen Delay runs. Output entropy is also very inconstnently going down, where usuually its more stable. Grad NORM i also think wasnt clipping??? I think that mightve gotten taken out i will have to chekc on that, but it did not vanish which is good. Since im doign a strict A/B test i wont cahnge the clipping at all to see but later might change it.

    RUN 2
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}
    LOOK BACK IN GITHUB FOR THE PREVIOUS RUN CONFIGS. not making new one just reusing 010.
    Reusing same configs, literally all i changed was i reverted back to the Xavier weighting.
    Wandb: https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/zddkoxqk?nw=nwuserocamp
    eval loss did decrease, however train loss and grad norm stayed very similar to eachother, therefore we can conclude that this is not the overarching issue. im hypothesising that the main issue has to do with adding back special tokens during training so for the next test i mask out special tokens during loss calculation.
    
    RUN 3
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}_{NoSpecialToks}
    masking out special tokens for loss calculations
    wandb: https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/yzr4nm4s
    train/loss was very similar to the previous 2 runs. eval/loss was higher but it was because it still was using special tokens in its loss calculation which was a mistake.
    
    RUN 4
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}_{NoSpecialToks}_{revertedto008}
    error after demo

    RUN 5
    20251127_dia_010_gpu_refactor_scratch_dataset_{Xavier}_{NoSpecialToks}_{revertedto008 + model.py fix}
    https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/groups/20251127_dia_010_gpu_refactor_scratch_dataset/runs/50ddhju7?nw=nwuserocamp

    still clipping

    RUN 6
    dia_010_run_6_added depth initialization back and fixed eval loss. running as a test to get a new baseline going and start building momentum again.
    https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/groups/20251127_dia_010_gpu_refactor_scratch_dataset/runs/9clr66zt?nw=nwuserocamp
    started clipping right after 1k steps

    RUN 7
    dia_010_run_7
    same as previous commit, but upped grad accum to 8 to hopefully ease the graidents
    off the start graidnets are already 1/3 of the starting graidient norms for RUN 6
    much better results i think, but slower training due t odepth initialization
    https://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/groups/20251127_dia_010_gpu_refactor_scratch_dataset/runs/ofzpqmwg?nw=nwuserocamp

    RUN 8
    dia_010_run_8
    same as last commit BUT copied in finetune_acc.py into train_acc_gpu.py and also copied over its layers.py and copied over dataset.py and also model.py as well
    my guess is that the issue was that i set grad_accum within jupyterlab but forgot to commmit the change so it didnt get tracked
    http://wandb.ai/ocamp-university-of-michigan/dia-music-finetuning/runs/adr0c9ar